{
 "metadata": {
  "name": "",
  "signature": "sha256:0e9918d336453acb1a9d558301b7b03edc3d7bb672e66794b3d71aaee9ecd9fc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import log\n",
      "import operator\n",
      "class TreeClassifier:\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    def __init__(self,criterion='entropy',max_features=None,max_depth=None,min_samples_split=1,min_samples_leaf=2):\n",
      "        self.criterion=criterion\n",
      "        self.max_features=max_features\n",
      "        self.max_depth=max_depth\n",
      "        self.min_samples_split=min_samples_split\n",
      "        self.min_samples_leaf=min_samples_leaf\n",
      "        self.tree={}\n",
      "        \n",
      "    def calEntropy(self,dataSet):\n",
      "        \"\"\"\n",
      "        cal the entropy of the dataset\n",
      "        \"\"\"\n",
      "        label_count={}\n",
      "        for data in dataSet:\n",
      "            if data[-1] not in label_count.keys():\n",
      "                label_count[data[-1]]=0\n",
      "            label_count[data[-1]]+=1\n",
      "        number_label=len(dataSet)\n",
      "        entropy=0\n",
      "        for label in label_count.keys():\n",
      "            prob=label_count[label]*1.0/number_label\n",
      "            entropy-=prob*log(prob,2)\n",
      "        return entropy\n",
      "    \n",
      "    def selectBestAttr(self,dataSet):\n",
      "        \"\"\"\n",
      "        select the best attribute\n",
      "        \"\"\"\n",
      "        #dataSet=np.concatenate((X,np.reshape(y,(len(y),1))),axis=1)\n",
      "        entropy_data=self.calEntropy(dataSet)\n",
      "        bestAttr_index=-1\n",
      "        bestAttr_entropy=-float('inf')\n",
      "        for attr_index in np.arange(dataSet.shape[1]-1):\n",
      "            values=set(dataSet[:,attr_index])\n",
      "            entropy_attr=0\n",
      "            split_infoValue=0\n",
      "            for value in values:\n",
      "                mask=dataSet[:,attr_index]==value\n",
      "                dataSet_attr=dataSet[mask][:,[attr_index,-1]]\n",
      "                \n",
      "                prob_attrdata=len(dataSet_attr)*1.0/len(dataSet)\n",
      "                \n",
      "                split_infoValue-=prob_attrdata*log(prob_attrdata,2)\n",
      "                entropy_attr+=self.calEntropy(dataSet_attr)*prob_attrdata\n",
      "                gainRatio=(entropy_data-entropy_attr)*1.0/split_infoValue\n",
      "            if gainRatio>bestAttr_entropy:\n",
      "                bestAttr_entropy=gainRatio\n",
      "                bestAttr_index=attr_index\n",
      "        return bestAttr_index\n",
      "    \n",
      "    def majorityCnt(self,classList):\n",
      "        \"\"\"\n",
      "        return the majority classLabel as the label\n",
      "        \"\"\"\n",
      "        classCount={}\n",
      "        for label in classList:\n",
      "            if label not in classCount.keys():\n",
      "                classCount[label]=0\n",
      "            classCount[label]+=1\n",
      "        sortedClassCount=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
      "        \n",
      "        return sortedClassCount[0][0]\n",
      "    \n",
      "    \n",
      "    def createTree(self,dataSet,feature_indexes):\n",
      "        \"\"\"\n",
      "        create tree recursive\n",
      "        \"\"\"\n",
      "        classList=[sample[-1] for sample in dataSet]\n",
      "        if classList.count(classList[0])==len(classList):\n",
      "            print classList[0]\n",
      "            return classList[0]\n",
      "        if dataSet.shape[1]==1:\n",
      "            print self.majorityCnt(classList)\n",
      "            return self.majorityCnt(classList)\n",
      "        \n",
      "        featBest=self.selectBestAttr(dataSet)\n",
      "        featBestIndex=feature_indexes[featBest]\n",
      "        #print featBest\n",
      "        valuesUnique_featBest=set(dataSet[:,featBest])\n",
      "        #print labels\n",
      "        del(feature_indexes[featBest])\n",
      "  \n",
      "        tree={featBestIndex:{}}\n",
      "        \n",
      "        for value in valuesUnique_featBest:\n",
      "            print value\n",
      "            mask=dataSet[:,featBest]==value\n",
      "            splitDataSet=np.delete(dataSet[mask],featBest,axis=1)\n",
      "            print splitDataSet.shape\n",
      "            subFeature_indexes=feature_indexes[:]\n",
      "            tree[featBestIndex][value]=self.createTree(splitDataSet,subFeature_indexes)\n",
      "        \n",
      "        return tree\n",
      "    \n",
      "    def fit(self,X,y):\n",
      "        \"\"\"\n",
      "        fit the train data, train it\n",
      "        \"\"\"\n",
      "        dataSet=np.concatenate((X,np.reshape(y,(len(y),1))),axis=1)\n",
      "        feature_indexes=list(np.arange(X.shape[1]))\n",
      "        #print feature_indexes\n",
      "        self.tree=self.createTree(dataSet,feature_indexes)\n",
      "        \n",
      "    def classify(self,tree,testVec):\n",
      "        firtFeature=tree.keys()[0]\n",
      "        secondDict=tree[firtFeature]\n",
      "        #print secondDict.keys()\n",
      "        classLabel=float('inf')\n",
      "        for key in secondDict.keys():\n",
      "            #print testVec[firtFeature],key,str(testVec[firtFeature])==key\n",
      "            if str(testVec[firtFeature])==key:\n",
      "                #print type(secondDict[key]).__name_\n",
      "                if type(secondDict[key]).__name__=='dict':\n",
      "                    classLabel=self.classify(secondDict[key],testVec)\n",
      "                else:\n",
      "                    classLabel=secondDict[key]\n",
      "        return classLabel\n",
      "    \n",
      "    def predict(self,test_X):\n",
      "        \"\"\"\n",
      "        predict every sample in the test data\n",
      "        return a array\n",
      "        \"\"\"\n",
      "        predictioins=[]\n",
      "        \n",
      "        for testVec in test_X:\n",
      "            predictioins.append(self.classify(self.tree,testVec))\n",
      "        \n",
      "        return predictioins"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test\n",
      "dataSet=np.array([[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']])\n",
      "tr=TreeClassifier()\n",
      "print tr.calEntropy(dataSet)\n",
      "print tr.selectBestAttr(dataSet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.970950594455\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_indexes=[0,1]\n",
      "s=tr.createTree(dataSet,feature_indexes)\n",
      "print s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "(3L, 2L)\n",
        "1\n",
        "(2L, 1L)\n",
        "yes\n",
        "0\n",
        "(1L, 1L)\n",
        "no\n",
        "0\n",
        "(2L, 2L)\n",
        "no\n",
        "{0: {'1': {1: {'1': 'yes', '0': 'no'}}, '0': 'no'}}\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[0]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "{'0': 'no', '1': {1: {'0': 'no', '1': 'yes'}}}"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr.fit(np.array([[1,1],[1,1],[1,0],[0,1],[0,1]]),['yes','yes','no','no','no'])\n",
      "\n",
      "print tr.tree\n",
      "\n",
      "tr.predict([[1,1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "(3L, 2L)\n",
        "1\n",
        "(2L, 1L)\n",
        "yes\n",
        "0\n",
        "(1L, 1L)\n",
        "no\n",
        "0\n",
        "(2L, 2L)\n",
        "no\n",
        "{0: {'1': {1: {'1': 'yes', '0': 'no'}}, '0': 'no'}}\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "['yes']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "tr.predict([[0,1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "['no']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DecisionTreeRegressor:\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    def __init__(self,criterion='mse',max_features=None,max_depth=None,min_samples_split=1,min_samples_leaf=4,error_threshold=1):\n",
      "        self.criterion=criterion\n",
      "        self.max_features=max_features\n",
      "        self.max_depth=max_depth\n",
      "        self.min_samples_split=min_samples_split\n",
      "        self.min_samples_leaf=min_samples_leaf\n",
      "        self.error_threshold=error_threshold\n",
      "        self.tree={}\n",
      "        \n",
      "    def calMSE(self,dataSet):\n",
      "        \"\"\"\n",
      "        cal the minimum square error of the dataset\n",
      "        \"\"\"\n",
      "        return np.var(dataSet[:,-1])*dataSet.shape[0]\n",
      "    \n",
      "    def calLeafValue(self,dataSet):\n",
      "        \"\"\"\n",
      "        decide which value to choose at the leaf,ie,give which value to the sample that belong the region\n",
      "        in the algorithm,i select the mean value of the dataSet in the segmenting region\n",
      "        \"\"\"\n",
      "        return np.mean(dataSet[:,-1])\n",
      "    \n",
      "    def splitDataset(self,dataSet,attr_index,value):\n",
      "        \"\"\"\n",
      "        split dataset according to the feature_index and value\n",
      "        \"\"\"\n",
      "       # print dataSet.shape,attr_index\n",
      "        return dataSet[dataSet[:,attr_index]<=value,:],dataSet[dataSet[:,attr_index]>value,:]#has some error when the datatype is np.mat  not np.array\n",
      "    \n",
      "    def selectBestSplit(self,dataSet,errorCal,valueCal):\n",
      "        \"\"\"\n",
      "        select the best attribute splitter\n",
      "        \"\"\"\n",
      "        #all the predictive values are same\n",
      "        if len(set(dataSet[:,-1]))==1:\n",
      "               return None,valueCal(dataSet)\n",
      "               \n",
      "        bestAttr_index=0\n",
      "        best_error=float('inf')\n",
      "        best_value=0\n",
      "        error=errorCal(dataSet)\n",
      "        for attr_index in np.arange(dataSet.shape[1]-1):\n",
      "            \n",
      "            values=set(dataSet[:,attr_index])\n",
      "            error_new=0\n",
      "            for value in values:\n",
      "                mat0,mat1=self.splitDataset(dataSet,attr_index,value)\n",
      "                if (len(mat0)<self.min_samples_split) or (len(mat1)<self.min_samples_split):\n",
      "                    continue\n",
      "                error_new=errorCal(mat0)+errorCal(mat1)\n",
      "                \n",
      "                if error_new<best_error:\n",
      "                    best_error=error_new\n",
      "                    bestAttr_index=attr_index\n",
      "                    best_value=value\n",
      "                    \n",
      "        if(error-error_new)<self.error_threshold:\n",
      "            return None,valueCal(dataSet)\n",
      "        \n",
      "        mat0,mat1=self.splitDataset(dataSet,bestAttr_index,best_value)\n",
      "        if (len(mat0)<self.min_samples_split) or (len(mat1)<self.min_samples_split):\n",
      "            return None,valueCal(dataSet)\n",
      "        \n",
      "        return bestAttr_index,best_value\n",
      "    \n",
      "    def createTree(self,dataSet,errorCal,valueCal):\n",
      "        \"\"\"\n",
      "        create tree recursive\n",
      "        \"\"\"\n",
      "        attr_index,val=self.selectBestSplit(dataSet,errorCal,valueCal)\n",
      "        \n",
      "        if attr_index==None:\n",
      "            return val\n",
      "        retTree={}\n",
      "        retTree['index']=attr_index\n",
      "        retTree['val']=val\n",
      "        ldataSet,rdataSet=self.splitDataset(dataSet,attr_index,val)\n",
      "        retTree['left']=self.createTree(ldataSet,errorCal,valueCal)\n",
      "        retTree['right']=self.createTree(rdataSet,errorCal,valueCal)\n",
      "        \n",
      "        return retTree\n",
      "    \n",
      "    def fit(self,X,y):\n",
      "        \"\"\"\n",
      "        fit the train data, train it\n",
      "        \"\"\"\n",
      "\n",
      "        dataSet=np.concatenate((X,np.reshape(y,(len(y),1))),axis=1)\n",
      "        dataSet.astype('float')\n",
      "        \n",
      "        errorCal=self.calMSE\n",
      "        valueCal=self.calLeafValue\n",
      "  \n",
      "        self.tree=self.createTree(dataSet,errorCal,valueCal)\n",
      "        \n",
      "    def treeForest(self,tree,testVec):\n",
      "        \"\"\"\n",
      "        return the regression value\n",
      "        \"\"\"\n",
      "        if testVec[tree['index']]<=tree['val']:\n",
      "            if type(tree['left']).__name__=='dict':\n",
      "                return self.treeForest(tree['left'],testVec)\n",
      "            else:\n",
      "                return float(tree['left'])\n",
      "        else:\n",
      "            if type(tree['right']).__name__=='dict':\n",
      "                return self.treeForest(tree['right'],testVec)\n",
      "            else:\n",
      "                return float(tree['right'])\n",
      "    \n",
      "    def predict(self,test_X):\n",
      "        \"\"\"\n",
      "        predict every sample in the test data\n",
      "        return a array\n",
      "        \"\"\"\n",
      "        predictioins=[]\n",
      "        \n",
      "        for testVec in test_X:\n",
      "            predictioins.append(self.treeForest(self.tree,testVec))\n",
      "        \n",
      "        return predictioins"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadData(filename):\n",
      "    dataMat=[]\n",
      "    fr=open(filename)\n",
      "    for line in fr.readlines():\n",
      "        curLine=line.strip().split('\\t')\n",
      "        fltLine=map(float,curLine)\n",
      "        dataMat.append(fltLine)\n",
      "    return dataMat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data=np.array(loadData('data/ex00.txt'))\n",
      "print len(np.unique(data[:,1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtrs=DecisionTreeRegressor()\n",
      "dtrs.fit(data[:,0:-1],data[:,-1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dtrs.tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'index': 0, 'right': 1.0180967672413792, 'val': 0.48813000000000001, 'left': -0.044650285714285719}\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_mat=np.array(loadData('data/bikeSpeedVsIq_train.txt'))\n",
      "test_mat=np.array(loadData('data/bikeSpeedVsIq_test.txt'))\n",
      "print train_mat.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(200L, 2L)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtrs=DecisionTreeRegressor(min_samples_split=20,error_threshold=1)\n",
      "dtrs.fit(train_mat[:,0:-1],train_mat[:,-1])\n",
      "y=dtrs.predict(test_mat)\n",
      "np.corrcoef(y,test_mat[:,1],rowvar=0)[0,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.97803079327040809"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a random dataset\n",
      "rng = np.random.RandomState(1)\n",
      "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
      "y = np.sin(X).ravel()\n",
      "y[::5] += 3 * (0.5 - rng.rand(16))\n",
      "\n",
      "# Fit regression model\n",
      "clf_1 = DecisionTreeRegressor()\n",
      "\n",
      "clf_1.fit(X, y)\n",
      "\n",
      "# Predict\n",
      "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
      "y_1 = clf_1.predict(X_test)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure()\n",
      "plt.scatter(X, y, c=\"k\", label=\"data\")\n",
      "plt.plot(X_test, y_1, c=\"g\", label=\"max_depth=2\", linewidth=2)\n",
      "\n",
      "plt.xlabel(\"data\")\n",
      "plt.ylabel(\"target\")\n",
      "plt.title(\"Decision Tree Regression\")\n",
      "plt.legend()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa1b0eb8>"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DecisionTreeClassifier:\n",
      "    \"\"\"\n",
      "    \"\"\"\n",
      "    def __init__(self,criterion='gini',max_features=None,max_depth=None,min_samples_split=1,min_samples_leaf=1):\n",
      "        self.criterion=criterion\n",
      "        self.max_features=max_features\n",
      "        self.max_depth=max_depth\n",
      "        self.min_samples_split=min_samples_split\n",
      "        self.min_samples_leaf=min_samples_leaf\n",
      "        #self.error_threshold=1\n",
      "        self.tree={}\n",
      "\n",
      "    def majorityCnt(self,classList):\n",
      "        \"\"\"\n",
      "        return the majority classLabel as the label\n",
      "        \"\"\"\n",
      "        classCount={}\n",
      "        for label in classList:\n",
      "            if label not in classCount.keys():\n",
      "                classCount[label]=0\n",
      "            classCount[label]+=1\n",
      "        sortedClassCount=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
      "        \n",
      "        return sortedClassCount[0][0]\n",
      "    \n",
      "            \n",
      "    def calGini(self,dataSet):\n",
      "        \"\"\"\n",
      "        cal the gini index of the dataset\n",
      "        \"\"\"\n",
      "        label_count={}\n",
      "        for data in dataSet:\n",
      "            if data[-1] not in label_count.keys():\n",
      "                label_count[data[-1]]=0\n",
      "            label_count[data[-1]]+=1\n",
      "        number_label=len(dataSet)\n",
      "        gini=0\n",
      "        for label in label_count.keys():\n",
      "            prob=label_count[label]*1.0/number_label\n",
      "            gini=prob*(1-prob)\n",
      "        return gini\n",
      "    \n",
      "    def splitDataset(self,dataSet,attr_index,value):\n",
      "        \"\"\"\n",
      "        split dataset according to the feature_index and value\n",
      "        \"\"\"\n",
      "       # print dataSet.shape,attr_index\n",
      "        return dataSet[dataSet[:,attr_index]<=value,:],dataSet[dataSet[:,attr_index]>value,:]#has some error when the datatype is np.mat  not np.array\n",
      "    \n",
      "    def get_randomIndex(self,max,num):\n",
      "        \"\"\"\n",
      "        generate num random value between 0 and max without replicated\n",
      "        \"\"\"\n",
      "        index_feature_generate=set()\n",
      "        while 1:\n",
      "            if len(index_feature_generate)>num:\n",
      "                break\n",
      "            index_feature_generate.add(np.random.randint(0,max))\n",
      "        return index_feature_generate\n",
      "    \n",
      "    def selectBestSplit(self,dataSet):\n",
      "        \"\"\"\n",
      "        select the best attribute splitter\n",
      "        \"\"\"\n",
      "        #all the predictive values are same\n",
      "        if len(set(dataSet[:,-1]))==1:\n",
      "               return None,dataSet[0,-1]\n",
      "               \n",
      "        bestAttr_index=-1\n",
      "        best_gini=float('inf')\n",
      "        best_value=0\n",
      "        num_dataSample=len(dataSet)\n",
      "        \n",
      "        if self.max_features!=None:\n",
      "            index_feature_generate=get_randomIndex(dataSet.shape[1])\n",
      "        else:\n",
      "            index_feature_generate=np.arange(dataSet.shape[1]-1)\n",
      "        \n",
      "        for attr_index in index_feature_generate:\n",
      "\n",
      "            values=set(dataSet[:,attr_index])\n",
      "            error_new=0\n",
      "            for value in values:\n",
      "                mat0,mat1=self.splitDataset(dataSet,attr_index,value)\n",
      "                if (len(mat0)<self.min_samples_split) or (len(mat1)<self.min_samples_split):\n",
      "                    continue\n",
      "                gini_split=float(len(mat0))/num_dataSample*self.calGini(mat0)+float(len(mat1))/num_dataSample*self.calGini(mat1)\n",
      "                \n",
      "                if gini_split<best_gini:\n",
      "                    best_gini=gini_split\n",
      "                    bestAttr_index=attr_index\n",
      "                    best_value=value\n",
      "       # if(error-error_new)<self.error_threshold:\n",
      "            #return None,,majorityCnt(dataSet[:,-1])\n",
      "        #print bestAttr_index\n",
      "        mat0,mat1=self.splitDataset(dataSet,bestAttr_index,best_value)\n",
      "        if (len(mat0)<self.min_samples_split) or (len(mat1)<self.min_samples_split):\n",
      "            #print 'once'\n",
      "            return None,self.majorityCnt(dataSet[:,-1])\n",
      "        \n",
      "        return bestAttr_index,best_value\n",
      "    \n",
      "    def createTree(self,dataSet,depth=0):\n",
      "        \"\"\"\n",
      "        create tree recursive\n",
      "        \"\"\"\n",
      "        if (self.max_depth!=None) and (depth>self.max_depth):\n",
      "            return self.majorityCnt(dataSet[:,-1])\n",
      "        \n",
      "        attr_index,val=self.selectBestSplit(dataSet)\n",
      "        #print ' attr_index,val:',attr_index,val,'\\n'\n",
      "        if attr_index==None:\n",
      "            return val\n",
      "        retTree={}\n",
      "        retTree['index']=attr_index\n",
      "        retTree['val']=val\n",
      "        retTree['depth']=depth\n",
      "        depth+=1\n",
      "        #print retTree['index'],retTree['val']\n",
      "        ldataSet,rdataSet=self.splitDataset(dataSet,attr_index,val)\n",
      "        retTree['left']=self.createTree(ldataSet,depth)\n",
      "        retTree['right']=self.createTree(rdataSet,depth)\n",
      "        \n",
      "        return retTree\n",
      "    \n",
      "    def fit(self,X,y):\n",
      "        \"\"\"\n",
      "        fit the train data, train it\n",
      "        \"\"\"\n",
      "\n",
      "        dataSet=np.concatenate((X,np.reshape(y,(len(y),1))),axis=1)\n",
      "        dataSet.astype('float')\n",
      "        \n",
      "        self.tree=self.createTree(dataSet)\n",
      "        return self\n",
      "        \n",
      "    def treeForest(self,tree,testVec):\n",
      "        \"\"\"\n",
      "        return the regression value\n",
      "        \"\"\"\n",
      "        #print testVec[int(tree['index'])],tree['val']\n",
      "        if testVec[int(tree['index'])]<=tree['val']:\n",
      "            if type(tree['left']).__name__=='dict':\n",
      "                return self.treeForest(tree['left'],testVec)\n",
      "            else:\n",
      "                return float(tree['left'])\n",
      "        else:\n",
      "            if type(tree['right']).__name__=='dict':\n",
      "                return self.treeForest(tree['right'],testVec)\n",
      "            else:\n",
      "                return float(tree['right'])\n",
      "    \n",
      "    def predict(self,test_X):\n",
      "        \"\"\"\n",
      "        predict every sample in the test data\n",
      "        return a array\n",
      "        \"\"\"\n",
      "        predictioins=[]\n",
      "        \n",
      "        for testVec in test_X:\n",
      "            predictioins.append(self.treeForest(self.tree,testVec))\n",
      "        \n",
      "        return predictioins"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr1=DecisionTreeClassifier()\n",
      "tr1.fit(np.array([[2,1],[3,1],[2,0],[3,1],[2,1]]),np.array([0,0,1,1,1]))\n",
      "\n",
      "#print tr.tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 100,
       "text": [
        "<__main__.DecisionTreeClassifier instance at 0x000000000E82BC88>"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tr1.tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'index': 1, 'depth': 0, 'right': {'index': 0, 'depth': 1, 'right': {'index': -1, 'depth': 2, 'right': 1, 'val': 0, 'left': 0}, 'val': 2, 'left': {'index': -1, 'depth': 2, 'right': 1, 'val': 0, 'left': 0}}, 'val': 0, 'left': 1}\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr1.predict([[1,1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "[1.0]"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "tr2=DecisionTreeClassifier(min_samples_split=2)\n",
      "iris = load_iris()\n",
      "# We only take the two corresponding features\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "tr2.fit(X,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 103,
       "text": [
        "<__main__.DecisionTreeClassifier instance at 0x000000000EA0F4C8>"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr2.tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "{'depth': 0,\n",
        " 'index': 3,\n",
        " 'left': {'depth': 1,\n",
        "  'index': 2,\n",
        "  'left': {'depth': 2,\n",
        "   'index': 2,\n",
        "   'left': 0.0,\n",
        "   'right': {'depth': 3,\n",
        "    'index': 0,\n",
        "    'left': 1.0,\n",
        "    'right': 1.0,\n",
        "    'val': 4.9000000000000004},\n",
        "   'val': 1.8999999999999999},\n",
        "  'right': {'depth': 2, 'index': 3, 'left': 2.0, 'right': 1.0, 'val': 1.5},\n",
        "  'val': 4.9000000000000004},\n",
        " 'right': {'depth': 1,\n",
        "  'index': 2,\n",
        "  'left': 2.0,\n",
        "  'right': 2.0,\n",
        "  'val': 4.7999999999999998},\n",
        " 'val': 1.7}"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c=tr2.predict(X)\n",
      "print len(y[c==y])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "147\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RandomForestClassifier:\n",
      "    \"\"\"\n",
      "    randomforesttree\n",
      "    \"\"\"\n",
      "    def __init__(self,n_estimators=10,criterion='gini',max_features=None,max_depth=None,min_samples_split=1,min_samples_leaf=1):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.n_estimators=n_estimators\n",
      "        self.criterion=criterion\n",
      "        self.max_features=max_features\n",
      "        self.max_depth=max_depth\n",
      "        self.min_samples_split=min_samples_split\n",
      "        self.min_sample_leaf=min_samples_leaf\n",
      "        self.tree_clfs=[]\n",
      "    \n",
      "    def boostrap_data(self,X,y):\n",
      "        \"\"\"\n",
      "        genrate data using boostrap method\n",
      "        \"\"\"\n",
      "        X_generate=[]\n",
      "        y_generate=[]\n",
      "        for i in np.arange(X.shape[0]):\n",
      "            index=np.random.randint(0,X.shape[0])\n",
      "            X_generate.append(X[index,:])\n",
      "            y_generate.append(y[index])\n",
      "        return np.array(X_generate),np.array(y_generate)\n",
      "    \n",
      "    def fit(self,X,y):\n",
      "        \"\"\"\n",
      "        fit the data\n",
      "        \"\"\"\n",
      "        for tree_index in np.arange(self.n_estimators):\n",
      "            clf=DecisionTreeClassifier(self.criterion,self.max_features,self.max_depth,self.min_samples_split,self.min_sample_leaf)\n",
      "            X_boostrap,y_boostrap=self.boostrap_data(X,y)\n",
      "            self.tree_clfs.append(clf.fit(X_boostrap,y_boostrap))\n",
      "            \n",
      "        return self\n",
      "    \n",
      "    def majorityLabel(self,classList):\n",
      "        \"\"\"\n",
      "        return the majority classLabel as the label\n",
      "        \"\"\"\n",
      "        classCount={}\n",
      "        for label in classList:\n",
      "            if label not in classCount.keys():\n",
      "                classCount[label]=0\n",
      "            classCount[label]+=1\n",
      "        sortedClassCount=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
      "        \n",
      "        return sortedClassCount[0][0]\n",
      "    \n",
      "    def predict(self,X):\n",
      "        \"\"\"\n",
      "        predict\n",
      "        \"\"\"\n",
      "        predictions=[]\n",
      "       \n",
      "        for tree_index in np.arange(self.n_estimators):\n",
      "            clf=self.tree_clfs[tree_index]\n",
      "            predictions.append(clf.predict(X))\n",
      "        results=[]\n",
      "        for prediction in np.array(predictions).T:\n",
      "            results.append(self.majorityLabel(np.array(prediction)))\n",
      "        \n",
      "        return np.array(results)\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_blobs\n",
      "\n",
      "\n",
      "from sklearn.metrics import log_loss\n",
      "\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate data\n",
      "X, y = make_blobs(n_samples=1000, n_features=5, random_state=42,\n",
      "                  cluster_std=5.0)\n",
      "X_train, y_train = X[:600], y[:600]\n",
      "X_valid, y_valid = X[600:800], y[600:800]\n",
      "X_train_valid, y_train_valid = X[:800], y[:800]\n",
      "X_test, y_test = X[800:], y[800:]\n",
      "\n",
      "# Train uncalibrated random forest classifier on whole train and validation\n",
      "# data and evaluate on test data\n",
      "clf = RandomForestClassifier(n_estimators=25)\n",
      "clf.fit(X_train_valid, y_train_valid)\n",
      "clf_probs = clf.predict(X_test)\n",
      "#score = log_loss(y_test, clf_probs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_probs.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 125,
       "text": [
        "(200L,)"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import precision_score\n",
      "print precision_score(y_test,clf_probs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.855913043478\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "clf1 = RandomForestClassifier(n_estimators=25)\n",
      "clf1.fit(X_train_valid, y_train_valid)\n",
      "clf_pre = clf1.predict(X_test)\n",
      "\n",
      "print precision_score(y_test,clf_pre)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.829703400347\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}